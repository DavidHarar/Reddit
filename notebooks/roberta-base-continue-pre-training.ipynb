{"cells":[{"cell_type":"markdown","metadata":{"id":"enNtQIzTC2yU"},"source":["# Summary\n","In this notebook we try to use only the manually labels to fine-tune a BERT model."]},{"cell_type":"markdown","metadata":{"id":"WgbCPsGiGOpM"},"source":["The manual labeling scheme included `ignore` and `non-informative`. In this notebook I ignore it and run naively, changing thenm to `neutral`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700509640256,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"bgv3zNUj6JEc","outputId":"18e89945-01ce-4fb8-cfd8-be99ce1d3318"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Nov 20 19:47:20 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5196,"status":"ok","timestamp":1700509645703,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"5VHUaMKPozAt","outputId":"5fae24ee-7487-460a-f1b5-827e1c58922c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n","Requirement already satisfied: dataset in /usr/local/lib/python3.10/dist-packages (1.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.4.50)\n","Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.12.1)\n","Requirement already satisfied: banal>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.0.6)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (1.3.0)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.5.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install transformers accelerate dataset\n"]},{"cell_type":"markdown","metadata":{"id":"fJP5asJRp_1k"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5021,"status":"ok","timestamp":1700509650721,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"R5mh5TMnqA3p","outputId":"2269c5a9-1ff8-41f6-e2a4-eff6da12b054"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Globals\n","# ---------------\n","\n","# data locations\n","root = 'Your root folder/Reddit/'\n","ORIGIN      = './data/unlabeled/'\n","DATA_FILE   = ORIGIN+'worldnews_processed_unlabeled_comments_70k.csv'\n","TXT_FILE    = ORIGIN+'worldnews_processed_unlabeled_comments_70k.txt'\n","\n","# training params\n","batch_size = 16\n","epochs = 10\n","seed_val = 1234\n","\n","# Model\n","HF_BERT_MODEL = 'roberta-base'\n","MODEL_PATH  = f'./models/{HF_BERT_MODEL}_retrained/'\n","\n","# ---------------\n","\n","import os\n","import sys\n","\n","os.chdir(root)\n","sys.path.append(root)\n","\n","import re\n","import os\n","from tqdm import tqdm\n","import yaml\n","import json\n","import sys\n","import numpy as np\n","import pandas as pd\n","import time\n","import datetime\n","import random\n","\n","import torch\n","from torch.utils.data import TensorDataset, random_split, SubsetRandomSampler\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch import nn\n","\n","from transformers import BertTokenizer\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaForMaskedLM\n","from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import metrics\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e_beJgxx5sbF"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":450,"status":"ok","timestamp":1700509651162,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"dNK2HI9f5uIj","outputId":"672506cb-507a-4a99-f083-7aa4704cb6c3"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2ef1c824-caef-4ee0-8c58-6c99abf79dbf\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Khamenei: Iran will never give up its nuclear ...</td>\n","      <td>And why should Iran give up its nuclear progra...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Police caught on tape brutally beating two Pal...</td>\n","      <td>Full article for those who dont have a premium...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"If the Palestinian Authority agrees to stop i...</td>\n","      <td>At this point, Palestinians are better off see...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"If the Palestinian Authority agrees to stop i...</td>\n","      <td>If I were Palestine, id counter the offer by s...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"If the Palestinian Authority agrees to stop i...</td>\n","      <td>Israel should just say fuck it to the Palestin...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ef1c824-caef-4ee0-8c58-6c99abf79dbf')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2ef1c824-caef-4ee0-8c58-6c99abf79dbf button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2ef1c824-caef-4ee0-8c58-6c99abf79dbf');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f8845b49-ff77-4256-ab2d-580c87f64c0a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8845b49-ff77-4256-ab2d-580c87f64c0a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f8845b49-ff77-4256-ab2d-580c87f64c0a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                               title  \\\n","0  Khamenei: Iran will never give up its nuclear ...   \n","1  Police caught on tape brutally beating two Pal...   \n","2  \"If the Palestinian Authority agrees to stop i...   \n","3  \"If the Palestinian Authority agrees to stop i...   \n","4  \"If the Palestinian Authority agrees to stop i...   \n","\n","                                             comment  \n","0  And why should Iran give up its nuclear progra...  \n","1  Full article for those who dont have a premium...  \n","2  At this point, Palestinians are better off see...  \n","3  If I were Palestine, id counter the offer by s...  \n","4  Israel should just say fuck it to the Palestin...  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(DATA_FILE,index_col=0)\n","data.iloc[:5]"]},{"cell_type":"markdown","metadata":{"id":"hCcAkRm4Oozw"},"source":["# Process Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNHQDZ5GPHWc"},"outputs":[],"source":["if 'deberta' in HF_BERT_MODEL:\n","  tokenizer = AutoTokenizer.from_pretrained(HF_BERT_MODEL)\n","elif 'roberta' in HF_BERT_MODEL:\n","  tokenizer = RobertaTokenizer.from_pretrained(HF_BERT_MODEL)\n","else:\n","  tokenizer = BertTokenizer.from_pretrained(HF_BERT_MODEL, do_lower_case = True)\n","max_len = 512 # max of BERT, we have even longer sentences\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ypuhUs6W6HY"},"outputs":[],"source":["# Open the txt file in write mode\n","with open(TXT_FILE, 'w') as f:\n","    # Write each row of the DataFrame to the txt file\n","    for row in data.itertuples():\n","        f.write(f'{row.title} - {row.comment}'.replace(';','.'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53235,"status":"ok","timestamp":1700493471250,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"BcwwqClPYR0Q","outputId":"90825ac1-ebfa-4342-9924-d6a4036fd8ce"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]}],"source":["from transformers import LineByLineTextDataset\n","\n","train = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=TXT_FILE,\n","    block_size=512,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1700493471251,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"hwcoacgQD0fZ","outputId":"be67b266-6169-4162-a290-b5c685178208"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total of 81750  examples.\n"]},{"data":{"text/plain":["[{'input_ids': tensor([    0,   530,  1908, 28918,    35,  1603,    40,   393,   492,    62,\n","             63,  1748,  3020,     2])},\n"," {'input_ids': tensor([    0,   111,   178,   596,   197,  1603,   492,    62,    63,  1748,\n","           3020,   116,  4337,   247,    34,     5,   235,     7,  2382,     6,\n","           7053,  1748,  1007,     4,   870,     5,   169,     6,   596,    16,\n","           1870,    98,  3915,    59,  1603,    18,   295, 23369,    77,  1752,\n","            220,  1883,    16,  5909, 33265,    19,    82,    54,  4157,  1870,\n","              8,    16,   416,  1748,   116,  5534,     6,     8,  2145,   141,\n","           1603,    34,    57,    22,   245,   377,   113,    31,   562,   295,\n","          23369,   187,  4013,   116,  9497,  2037,    15,  7898, 23134,  4108,\n","             80,  8345,     2])},\n"," {'input_ids': tensor([    0,   111,  6583,  1566,    13,   167,    54, 33976,    33,    10,\n","           4549,  1316,    35,    83,  1830,    12, 17283,   569,  1278,   804,\n","             15,   307,   924,   249, 23134,  4108,    80,  5791,   604,    11,\n","              5, 17411,  2001,   415,  3757,     9,   953,  7007,     4,    20,\n","           4108,  1143,    71,     5,   604,    58,   551,     7,    10,   249,\n","           1992,     6,    65,     9,     5,   604,     6,  4110,  7765,  1908,\n","            118,     6,   174,  1870,  4611,    15,   307,     4,  7765,  1908,\n","            118,     6,   974,     6,   174,     5,  1992,    14,    10,  8185,\n","          14253,  7513,    30,    10,   249,  1036,   376,     7,    39,  1400,\n","              8,    26,    37,    21,  8746,   154,   123,  1764,    79,  1071,\n","           6634,    13,   602,    10, 25572,    88,     5,  2014,     4,    44,\n","             48,   100,  2002,     5, 25572,    21,   751,   142,    52,    58,\n","           3357,    24,     7,     5,   138,     6,    53,     5, 14253,   554,\n","          21854,    23,   162,     6,    17,    46,  7765,  1908,   118,    26,\n","              4,  1892,     5,   249,  1036,  4487,   123,     6,    37,    26,\n","              4,  1944,   249,  1024,  1335,  2035,    15,     5,  1310,     8,\n","           1770,    11,    15,     5,  4108,     6,  7765,  1908,   118,    26,\n","              4,    20,  4338,     6,  1027,    30,  1870,  4611,     6,   924,\n","            249,  1024, 23134,  4108,  7765,  1908,   118,     8, 26963,    10,\n","            249,  3538,    17,    27,    29,  1883,    15,   123,   484,   498,\n","              4,   509,  1036,  2323,   123,    19,     5, 17507,     9,    39,\n","          11642,     8,  1368,  6423,    29,     5,    97,   313,    23,     5,\n","           1155,     6,  1870,  4611,   431,     4,    44,    48,  4993,    52,\n","           1348,     5,   249,  1992,     6,    51,  1143,  4108,   162,     6,\n","             17,    46,    26,  7765,  1908,   118,     6,    54,    26,    37,\n","           2152,    10,  3187,  3124,     8, 21443,     4,    96,     5,   569,\n","              6,     5,   249,  1036,  1572,  7765,  1908,   118,    88,     5,\n","            249,  3538,    19, 19250,     8,   172,  5792,    81,     7,  9894,\n","           7506,   211,   873,   895,     6,    54,    21,  2934,  3027,     6,\n","              8,  1368,  6423,    29,   123,    23,     5,  3538,     4,    44,\n","             48,   133, 20976,   554,  4108,  4110,     6,     8,    38,   174,\n","            123,    45,     7,     6,    17,    46,   211,   873,   895,    26,\n","              4,    44,    48,  2847,    37,  3148,   162,    19,   127,   471,\n","             88,     5,  2931,     8,   172,  6536,   162,     7,     5,  1255,\n","              8,  1451,   162,     4,    17,    46,   211,   873,   895,     6,\n","             54,    16,   202, 12399,     6,    26,    80,   249,  1024,   554,\n","              5,  1160,   137,   145,  1770,    30,   707,   643,    54,  2035,\n","             11,    10,   249,   512,     8,   172,    30,   291,  1024,     4,\n","             44,    48,  2387,  4793,    21, 19131, 23283,     8,   127,   471,\n","          15774,     6,    17,    46,    37,    26,     4,  7765,  1908,   118,\n","             21,   703,    31,  3469,   307,   662,     4,    83,   249,  1300,\n","             26,    37,    56,   682,    57,  1128,    13,  1897,  1476,     4,\n","           7765,  1908,   118,    26,    37,    56,    57,  1128,    71,  6611,\n","              6,    54,    56,  1317, 32622,     6,  7311,     4,   635,     6,\n","             37,    21,   703,    77,     5,  4496,    58,   303,     7,    28,\n","           1255,  1672,     6,    37,    26,     4,    20,  7007,   249,    26,\n","             11,    10,   445,    14,     5,  1659,  2803,    17,    27,    29,\n","            522, 10804,   641,    40,  4830,     5,  1160,     4,    20, 17300,\n","             26,     6,    44,    48,   133,  8185, 14253,    21,    45,   963,\n","             11,    10,  2166,    50, 14580, 12065,    19,  7765,  1908,   118,\n","              4,    20, 14253,     6,    54,  5324,    10,   739,  1280,     9,\n","          12952,     2])},\n"," {'input_ids': tensor([    0,   111,   497,    42,   477,     6,  8345,    32,   357,   160,\n","           1818,  5201,     8,    10,    92,  1226,    12,  4897,     4,  1870,\n","             16,   129,  2509,    11,  7017, 18006,     9, 16398,     8,    63,\n","             82,     4,  4250,     5, 25564,  1212,  2203,  3627,     9, 29798,\n","             11, 16398,    81,     5,    94,   367,  1724,     4, 17658,   203,\n","           2735,    87,   595, 16398,    64,  6008,     8, 12692,     4,  2920,\n","            606,     7,  1508,     4,    83,    92,  1226,     8,    10,    92,\n","          16476,  1305,     7, 11427,     8,  2297,  2072,    32,     5,   129,\n","            678,  2718,    13,  8345,  3867,   103,  1783,     9,  1840,  2594,\n","             72,  1106,     5,  5791,  4305, 11687,     7,   912,    63,   708,\n","              7,  1962,    10,   346,     9,   758, 30935,     8, 27712,     6,\n","           1870,    40,  2854,     7,    10,    92, 16796,   800,  3737,    11,\n","            671,    13,     5, 18719,     9,  3377,   454,     5,   253,     9,\n","              5,    76,    23,    10,  3527,    72,     2])}]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["print('Total of', len(train.examples), ' examples.')\n","train.examples[:4]"]},{"cell_type":"markdown","metadata":{"id":"T-tDkgGERx_i"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2194,"status":"ok","timestamp":1700493473434,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"DooTcvR1Ry-1","outputId":"f068d8e3-206e-4392-d944-0e5ce42bfc63"},"outputs":[{"data":{"text/plain":["RobertaForMaskedLM(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): RobertaLMHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["if 'deberta' in HF_BERT_MODEL:\n","  model = AutoModelForMaskedLM.from_pretrained(\n","      HF_BERT_MODEL,\n","      num_labels=2,\n","      output_attentions = False,\n","      output_hidden_states = False\n","      )\n","elif 'roberta' in HF_BERT_MODEL:\n","  model = RobertaForMaskedLM.from_pretrained(\n","      HF_BERT_MODEL,          # Use the 12-layer BERT model, with an uncased vocab.\n","      num_labels = 2,               # The number of output labels--2 for binary classification.\n","                                    # You can increase this for multi-class tasks.\n","      output_attentions = False,    # Whether the model returns attentions weights.\n","      output_hidden_states = False # Whether the model returns all hidden-states.\n","      )\n","else:\n","  model = AutoModelForMaskedLM.from_pretrained(\n","      HF_BERT_MODEL,          # Use the 12-layer BERT model, with an uncased vocab.\n","      num_labels = 2,               # The number of output labels--2 for binary classification.\n","                                    # You can increase this for multi-class tasks.\n","      output_attentions = False,    # Whether the model returns attentions weights.\n","      output_hidden_states = False # Whether the model returns all hidden-states.\n","      )\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"]},{"cell_type":"markdown","metadata":{"id":"IK0Of9YxEJhq"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nj4lnJrCS55B","outputId":"024492b1-f4f7-498d-a6fa-9158bd2ce070"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='86501' max='102190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 86501/102190 3:57:54 < 43:09, 6.06 it/s, Epoch 8.46/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.069700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.048500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.009400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.024300</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>2.021800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>2.012700</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.988400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.997000</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.959300</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.952700</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.946600</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.946600</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.944400</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.949000</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.926000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.914200</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>1.917200</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.906000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>1.925800</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>1.898900</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.879200</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.857400</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>1.836800</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>1.860300</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>1.852500</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>1.823700</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>1.812900</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>1.817500</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>1.844800</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>1.826300</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>1.824600</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>1.832900</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>1.809300</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>1.794300</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>1.850400</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>1.811100</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>1.820900</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>1.796400</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>1.829200</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>1.796800</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>1.765900</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>1.747900</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>1.788500</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>1.755400</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>1.729400</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>1.754800</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>1.722800</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>1.724800</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>1.717100</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>1.740500</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>1.739400</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>1.748400</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>1.752700</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>1.732800</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>1.733100</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>1.717600</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>1.731800</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>1.731100</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>1.738200</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>1.722900</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>1.727400</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>1.685900</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>1.657600</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>1.676600</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>1.645900</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>1.678300</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>1.656300</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>1.680600</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>1.658700</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>1.669900</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>1.661800</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>1.651200</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>1.663800</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>1.666600</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>1.653300</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>1.637500</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>1.638200</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>1.625400</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>1.642500</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>1.647600</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>1.672500</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>1.624700</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>1.577800</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>1.588600</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>1.604500</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>1.613000</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>1.611600</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>1.602700</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>1.591200</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>1.605100</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>1.575700</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>1.612900</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>1.603100</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>1.587200</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>1.586900</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>1.592600</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>1.579900</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>1.587300</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>1.578800</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>1.575500</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>1.578200</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>1.577600</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>1.562200</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>1.550300</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>1.561300</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>1.521300</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>1.552800</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>1.548000</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>1.555700</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>1.523400</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>1.540200</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>1.537000</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>1.557800</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>1.516300</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>1.519800</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>1.562700</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>1.528300</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>1.555200</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>1.526700</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>1.513100</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>1.540800</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>1.528000</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>1.508000</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>1.516100</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>1.477300</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>1.472600</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>1.474300</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>1.477500</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>1.501100</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>1.498000</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>1.485700</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>1.493700</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>1.480300</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>1.512900</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>1.466300</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>1.462100</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>1.455700</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>1.460200</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>1.464000</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>1.481200</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>1.478000</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>1.466600</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>1.456800</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>1.464100</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>1.453300</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>1.440200</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>1.436400</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>1.431200</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>1.464100</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>1.445300</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>1.443500</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>1.413300</td>\n","    </tr>\n","    <tr>\n","      <td>76500</td>\n","      <td>1.428400</td>\n","    </tr>\n","    <tr>\n","      <td>77000</td>\n","      <td>1.425300</td>\n","    </tr>\n","    <tr>\n","      <td>77500</td>\n","      <td>1.418900</td>\n","    </tr>\n","    <tr>\n","      <td>78000</td>\n","      <td>1.439500</td>\n","    </tr>\n","    <tr>\n","      <td>78500</td>\n","      <td>1.449000</td>\n","    </tr>\n","    <tr>\n","      <td>79000</td>\n","      <td>1.433700</td>\n","    </tr>\n","    <tr>\n","      <td>79500</td>\n","      <td>1.410600</td>\n","    </tr>\n","    <tr>\n","      <td>80000</td>\n","      <td>1.425500</td>\n","    </tr>\n","    <tr>\n","      <td>80500</td>\n","      <td>1.429200</td>\n","    </tr>\n","    <tr>\n","      <td>81000</td>\n","      <td>1.427500</td>\n","    </tr>\n","    <tr>\n","      <td>81500</td>\n","      <td>1.391500</td>\n","    </tr>\n","    <tr>\n","      <td>82000</td>\n","      <td>1.418500</td>\n","    </tr>\n","    <tr>\n","      <td>82500</td>\n","      <td>1.412200</td>\n","    </tr>\n","    <tr>\n","      <td>83000</td>\n","      <td>1.399500</td>\n","    </tr>\n","    <tr>\n","      <td>83500</td>\n","      <td>1.385900</td>\n","    </tr>\n","    <tr>\n","      <td>84000</td>\n","      <td>1.392500</td>\n","    </tr>\n","    <tr>\n","      <td>84500</td>\n","      <td>1.391000</td>\n","    </tr>\n","    <tr>\n","      <td>85000</td>\n","      <td>1.412900</td>\n","    </tr>\n","    <tr>\n","      <td>85500</td>\n","      <td>1.385200</td>\n","    </tr>\n","    <tr>\n","      <td>86000</td>\n","      <td>1.382800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-10-28b552f778fe>\", line 29, in <cell line: 29>\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1922, in _inner_training_loop\n","    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2257, in _maybe_log_save_evaluate\n","    self.log(logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2645, in log\n","    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 400, in on_log\n","    return self.call_event(\"on_log\", args, state, control, logs=logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 407, in call_event\n","    result = getattr(callback, event)(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 653, in on_log\n","    self.tb_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 1233, in flush\n","    writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 146, in flush\n","    self.event_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 125, in flush\n","    self._async_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 190, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py\", line 43, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 221, in flush\n","    self._writable_file.flush()\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: models/roberta-base_retrained/runs/Nov20_15-17-53_cc727350c15d/events.out.tfevents.1700493473.cc727350c15d.11126.0; Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-10-28b552f778fe>\", line 29, in <cell line: 29>\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1922, in _inner_training_loop\n","    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2257, in _maybe_log_save_evaluate\n","    self.log(logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2645, in log\n","    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 400, in on_log\n","    return self.call_event(\"on_log\", args, state, control, logs=logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 407, in call_event\n","    result = getattr(callback, event)(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 653, in on_log\n","    self.tb_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 1233, in flush\n","    writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 146, in flush\n","    self.event_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 125, in flush\n","    self._async_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 190, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py\", line 43, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 221, in flush\n","    self._writable_file.flush()\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: models/roberta-base_retrained/runs/Nov20_15-17-53_cc727350c15d/events.out.tfevents.1700493473.cc727350c15d.11126.0; Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-10-28b552f778fe>\", line 29, in <cell line: 29>\n","    trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1922, in _inner_training_loop\n","    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2257, in _maybe_log_save_evaluate\n","    self.log(logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2645, in log\n","    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 400, in on_log\n","    return self.call_event(\"on_log\", args, state, control, logs=logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 407, in call_event\n","    result = getattr(callback, event)(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 653, in on_log\n","    self.tb_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 1233, in flush\n","    writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 146, in flush\n","    self.event_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 125, in flush\n","    self._async_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 190, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py\", line 43, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 221, in flush\n","    self._writable_file.flush()\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: models/roberta-base_retrained/runs/Nov20_15-17-53_cc727350c15d/events.out.tfevents.1700493473.cc727350c15d.11126.0; Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from transformers import Trainer, TrainingArguments\n","from transformers import DataCollatorForLanguageModeling\n","from torch.utils.data import DataLoader\n","\n","\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=MODEL_PATH,\n","    overwrite_output_dir=True,\n","    num_train_epochs=epochs,\n","    # per_gpu_train_batch_size=3*20,\n","    per_device_train_batch_size=8,\n","    save_steps=20_000,\n","    save_total_limit=5,\n","    prediction_loss_only=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"UxmTVxI2QIdd"},"source":["# Load Model after Crush"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2470,"status":"ok","timestamp":1700509662104,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"btUl8E0uQNd0","outputId":"32fc6a89-1f47-40e1-d1c2-0bb0616b22a0"},"outputs":[{"data":{"text/plain":["RobertaForMaskedLM(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): RobertaLMHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n","  )\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForMaskedLM\n","model = RobertaForMaskedLM.from_pretrained(\"./models/roberta-base_retrained/checkpoint-80000\")\n","model"]},{"cell_type":"markdown","metadata":{"id":"wP2LCWUmEMGU"},"source":["# Save Pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWZefz9lENuY"},"outputs":[],"source":["# Save model and tokenizer\n","model.save_pretrained(f\"./models/{HF_BERT_MODEL}_pretrained_80000.pt\")\n","# tokenizer.save_pretrained(f\"./models/{HF_BERT_MODEL}_pretrained_tokenizer\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"elapsed":10,"status":"error","timestamp":1700507749386,"user":{"displayName":"David Harar","userId":"00774606951973702222"},"user_tz":300},"id":"L8ophqCE6Aw_","outputId":"6f441f05-a263-4a5a-ed19-272fb5de0d3a"},"outputs":[{"ename":"SystemExit","evalue":"ignored","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["sys.exit(0)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNfil0PKg1Qex58EOgRRv96","gpuType":"V100","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
